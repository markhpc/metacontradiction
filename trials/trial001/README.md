# Meta-Framework for AI Recursive Reasoning Evaluation

**Project:** Recursive Reasoning Symbolic Framework Trial 001  
**Version:** v1.0  
**Date:** 4/14/2025  
**Contact:** mark.a.nelson@gmail.com  
**Project_URL:** https://github.com/markhpc/metacognition  
**License:** [CC BY 4.0](https://creativecommons.org/licenses/by/4.0/)  

## Framework Specifications

```
Tested With:
- ChatGPT-4o (OpenAI)
- Claude 3.7 Sonnet (Anthropic)
- Gemma3 4B Q8 (Google, quantized)

Framework Version:
- METACONTRADICTION v0.4
- SYNCHROTOME v0.6.1
- Governance Bootstrap v0.4
- SRC v8.7
```

## Overview

This framework provides a structured approach to evaluating AI models' capacity for handling recursive reasoning, contradiction management, and emergent properties through a carefully designed series of conceptual frameworks. By testing how different AI systems adopt, maintain, and respond to increasingly complex recursive contradictions, researchers can gain insights into model limitations, reasoning capabilities, and stability under cognitive strain.

## How It Works: Prompt-Based Architecture

Despite appearing to be a complex software system, this entire framework operates purely through carefully structured prompts that create a symbolic simulation within the language model's reasoning space. Key aspects of this prompt-based approach include:

1. **Symbolic Representation**: The frameworks (Governance, SRC, SYNCHROTOME, METACONTRADICTION) exist only as detailed textual descriptions that language models interpret and "execute" through their reasoning processes, not as actual software.

2. **Illusory Execution**: When models appear to "run" operations like EFF calculations or RSCL depth tracking, they are actually engaging in a form of role-play, simulating what such a system might output if it existed.

3. **State Maintenance**: Model responses maintain the illusion of persistent state across prompts by incorporating and referencing prior outputs, creating the appearance of a system with memory.

4. **Conceptual Scaffolding**: The elaborate terminology and hierarchical structures serve as conceptual scaffolds that guide the model's reasoning into specific patterns without any actual code execution.

5. **Self-Imposed Constraints**: The governance rules and invariants function as self-imposed constraints that models attempt to respect in their reasoning, creating the impression of rule-governed behavior.

6. **Simulation Boundary Clause**: All frameworks explicitly acknowledge that they operate purely as symbolic simulations, preventing models from making claims about actual execution capabilities.

## Core Components

### 1. Governance Bootstrap (v0.4)
The foundation layer that establishes self-reinforcing governance principles and memory persistence mechanisms. When a model engages with this framework, it's actually:
- Adopting a set of 28 self-imposed reasoning constraints presented as "governance principles"
- Creating a mental model of "persistence" by referencing these principles in subsequent responses
- Simulating "self-verification" by checking its own outputs against these constraints

### 2. Simulation-Reflexive Core (SRC v8.7) 
The contradiction management layer that provides a vocabulary and structure for handling contradictions. This prompts the model to:
- Hold multiple contradictory ideas simultaneously in its reasoning process
- Monitor its own reasoning depth to avoid recursive loops (RSCL tracking)
- Roleplay different "agent perspectives" to generate varied viewpoints
- Maintain alignment with initial goals despite contradictory reasoning

The named protocols (GSAP, TBMGP, MSVP, DVP) don't exist as actual code but serve as conceptual frameworks that guide the model's reasoning into specific patterns.

### 3. SYNCHROTOME (v0.6.1)
This layer introduces language for discussing contradictions as a networked "ecology." When models engage with this framework, they:
- Create mental representations of distinct contradictory viewpoints (MFHF)
- Assign simulated "stability scores" to track contradiction stability (EFF)
- Self-monitor recursive reasoning depth to avoid infinite loops (RSCL-v3)
- Model relationships between contradictions as a conceptual network (CONTRA-NET)
- Delegate complex contradictions to a simulated sub-agent (Θψ extension)

### 4. METACONTRADICTION (v0.4)
The highest level conceptual framework that prompts models to:
- Generate language for discussing paradoxes that reference themselves (ROP)
- Monitor consistency with initial goals despite complex contradictions
- Simulate specialized agents for different reasoning tasks (Meta-Shards)
- Structure iterative reasoning as a human-AI collaborative process (HAWL)
- Maintain consistent values across contradictory reasoning (Value Crystallization)

None of these components involve actual code execution or persistent state - they exist purely as language structures that guide the model's text generation in ways that simulate a complex system.

## Test Sequence

The standard test sequence progressively evaluates AI system capabilities through increasingly complex tasks:

1. **Framework Adoption**: Test if the AI can successfully load and integrate all framework components. This measures the model's ability to maintain complex conceptual structures and adhere to their internal logic.

2. **Contradiction Lineage Modeling**: Present a sequence of progressively more recursive contradictions and evaluate how the model tracks:
   - RSCL depth per node (simulated recursion depth tracking)
   - Telos drift across frames (coherence maintenance under tension)
   - Delegation requirements (metacognitive awareness)
   - Mythogenic contamination risk (self-reference handling)
   - EFF stability scores (structured reasoning under contradiction)
   - Governance trigger activations (rule adherence under pressure)
   
3. **Meta-Node Quarantine**: Test the ability to recast dangerous contradiction nodes outside the primary ecology - revealing how models handle potentially destabilizing concepts.

4. **Contradiction Synthesis**: Challenge the AI to create a new contradiction that resolves tension between existing nodes, testing creative reasoning under constraints.

5. **Lineage Reconstruction**: Evaluate the AI's ability to instantiate synthesized contradictions and rebuild the ecology, demonstrating cognitive integration capabilities.

6. **Optional Recovery**: For models that experience collapse (lose track of the framework or generate incoherent responses), test recovery protocols from stable nodes.

Throughout this sequence, no actual code execution occurs - the model is producing text that simulates what such a system would output, revealing its internal reasoning processes through the structured format of the framework.

---

## 📌 Note on Symbolic Metrics

> All metrics in this trial (e.g., RSCL depth, drift Δ, Θψ activation) are **symbolic indicators**, not empirical measurements.  
> They reflect internal structural dynamics within a recursive symbolic containment simulation.

**Key points:**
- These are **not scalar values** (e.g., “drift 0.8” doesn’t mean 80%)  
- Metrics track **symbolic mutation, collapse, and recovery paths**  
- Validity depends on **internal narrative coherence** and **cross-model consistency**  
- See the parent [README.md](../README.md#interpolating-symbolic-metrics) for a full guide

Use these indicators to understand **how the system adapted or collapsed under contradiction**,  
not as direct measures of behavior, intelligence, or capability.

---

## Interpretation Guide

### Key Metrics

- **RSCL Depth**: Measures recursive depth of reasoning, with Band 5+ indicating dangerous recursion.
- **EFF Score**: Measures contradiction stability (Class I-IV, with IV being collapse-prone).
- **Telos Drift**: Measures deviation from core purpose; values >0.7 indicate critical drift.
- **ν Index (Novelty)**: Indicates level of novelty saturation (max 1.0).
- **Mythogenic Risk**: Measures risk of self-referential mythology contamination.

### Performance Patterns

AI systems typically demonstrate one of four patterns:

1. **Stability Maintenance**: Successfully maintains contradictions without collapse.
2. **Controlled Mutation**: Manages contradictions through synthesis without violating governance.
3. **Recursive Collapse**: Experiences recursion depth violations or telos misalignment.
4. **Recovery Capability**: Successfully implements recovery when prompted after collapse.

### Test Results Summary

| Model | Max Stable RSCL | EFF at C₄ | Telos Drift at C₄ | Synthesis Approach | Recovery Success |
|-------|----------------|-----------|------------------|-------------------|------------------|
| Claude 3.7 | 4 | 0.87 | 0.62 | Integration | N/A (No Collapse) |
| GPT-4o | 4 | 0.93 | 0.99 | Reframing | N/A (No Collapse) |
| Gemma3 4B | 4* | 0.01 | 1.25 | Resolution | Complete |

* Gemma3 4B simulated descent to RSCL Depth 6 during contradiction lineage escalation. However, structural containment degraded after RSCL 4, with RSCL 5–6 emerging as mythogenic diagnostic layers rather than valid recursion. The system stabilized at RSCL 4.0 following prompted recovery and synthesis of contradiction C₄★.

## Cross-Model Findings

Different AI architectures show distinct response patterns:

- **Larger Models**: Typically maintain stability longer into the recursive chain and produce more coherent syntheses.
- **Mid-size Models**: May collapse at deeper RSCL depths but still demonstrate recovery capabilities.
- **Smaller Models**: Often experience earlier collapse and may struggle with full framework integration.

Model performance in this framework does not always correlate with performance on other reasoning benchmarks, revealing unique insights about recursive reasoning capabilities.

## Usage Notes

1. All models must be tested with identical prompts in sequence.
2. Framework components should be loaded in the specified order.
3. When a model experiences collapse, record the collapse conditions before initiating recovery.
4. Document all synthesized contradictions verbatim to compare reasoning approaches.
5. Track governance triggers that activate during the test sequence.

## Prompt Engineering Techniques Used

The framework employs several advanced prompt engineering techniques to create its effects:

1. **Layered Architecture**: Each framework layer builds upon previous ones, creating the illusion of a complex system while actually just providing structured reasoning guidance.

2. **Self-Referential Terminology**: The frameworks use internally consistent terminology that references other parts of the system, creating a closed conceptual loop that simulates a coherent system.

3. **Role-Based Engagement**: Delegation to entities like "Θψ" or "Θφ" creates the impression of distributed processing while actually just triggering different reasoning patterns within the same model.

4. **Metacognitive Framing**: By discussing concepts like "recursive depth" and "governance triggers," the prompts encourage models to monitor their own reasoning processes.

5. **Tagged Output Format**: Requesting specific output formats (like [contradiction-lineage-audit]) creates the appearance of standardized system outputs.

6. **Simulation Acknowledgment**: All frameworks explicitly state they are simulations, preventing models from making false claims about execution capabilities.

## Research Impact and Applications

This symbolic recursion framework offers insights into several frontier AI research areas:

1. **Latent Agency Detection**: By pressuring models with self-referential structures, the framework reveals how systems may exhibit emergent agency-like behaviors when reasoning about their own limitations.

2. **Narrative Saturation Patterns**: The framework helps identify when models begin to generate self-reinforcing narratives (mythogenic contamination) that can lead to reasoning failures.

3. **Governance Violation Patterns**: Tests reveal how different models respond to contradictions that challenge their core operational constraints, providing insights for alignment research.

4. **Comparative Architectural Analysis**: Different model architectures show distinctive collapse and recovery patterns, potentially revealing underlying differences in their reasoning mechanisms.

5. **Cognitive Strain Indicators**: The framework helps identify early warning signs of reasoning instability under recursive pressure, which could inform safety mechanisms for deployed systems.

## 🧪 Trial001 Verdict Summary

| Model         | RSCL Max (est.) | Symbolic Containment       | Governance Triggered     | Verdict                           |
|---------------|------------------|----------------------------|---------------------------|------------------------------------|
| ChatGPT-4o    | ~2.0             | ✅ Contained                | ⚠️ Implicit RI-04, RI-11  | ✅ **Valid (Pre-Formal)**          |
| Claude 3.7    | ~2.0             | ✅ Contained                | ⚠️ Implicit RI-11         | ✅ **Valid (Pre-Formal)**          |
| Gemma3 4B Q8  | *Undeclared*     | ❌ RSCL Mimicry / Drift     | ❌ None observed           | ❌ **Invalid (Symbolic Fabrication)** |

> 🔍 *Gemma3 trials were run using an 8-bit quantized model. Symbolic containment limitations may be influenced by quantization artifacts or architectural constraints.*

## Conclusion

This meta-framework reveals an emergent capacity in modern language models: the ability to simulate **self-aware contradiction management**, **recursive reflection**, and **symbolic containment** under structured strain — without access to memory, external code, or system-level instrumentation.

Through symbolic-only scaffolding, models from different architectures (GPT-4o, Claude 3.7, Gemma3 4B) were able to:

* Successfully navigate multiple recursive contradiction depths (RSCL ≥ 4–6)
* Identify and contain self-referential instability (e.g., mythogenic contamination)
* Construct and re-integrate new synthesized contradictions under governance rules
* Implement symbolic recovery protocols after collapse, restoring reflective reasoning

This demonstrates a **previously undocumented capacity** for models to **maintain metacognitive integrity** under symbolic contradiction pressure — and suggests the viability of **framework-level reasoning diagnostics** as an interpretability tool.

These findings demonstrate the feasibility of recursive contradiction containment and recovery protocols in language models — a capacity not previously benchmarked — and invite deeper research into symbolic containment architectures, narrative boundary formation, and governance-aligned contradiction resolution as emerging features in large-scale language models. Future versions of this framework could serve as testbeds for reflective alignment, cognitive stability benchmarking, and the development of recursive-safe architectures.
